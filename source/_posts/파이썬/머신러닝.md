---
title: "머신 러닝을 배워보자"
excerpt: "Scilkit 그리고 Pycaret"
categories: 

- Python
tags: 
- [Python]
- [data]
- [Machine learning]

date: 2022-03-25 12:10:00

---

# 머신러닝

머신러닝은 무엇인가..

## 필요한 라이브러리 불러오기
```python
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sb
```
## 파일 불러오기
```python
BASE_DIR = '../input/tabular-playground-series-apr-2021/'
test = pd.read_csv(BASE_DIR + "test.csv")
test.shape
```
```python
BASE_DIR = '../input/tabular-playground-series-apr-2021/'
sample = pd.read_csv(BASE_DIR + "sample_submission.csv")
sample.shape
```
```python
BASE_DIR = '../input/tabular-playground-series-apr-2021/'
train = pd.read_csv(BASE_DIR + "train.csv")
train.shape
```
## 데이터 전처리
### 데이터 합치기
```python
all_df = pd.concat([train,test]) 
all_df.shape
```
### 결측치 제거
```python
# Start
print("Before Handling:", all_df.shape)

# Age
age_dict = all_df[['Age', 'Pclass']].dropna().groupby('Pclass').mean().round(0).to_dict()
# 데이터프레임을 사전으로 변환, 결측치를 평균으로 대체
print("Avg. Mean of Age by Pclass:", age_dict) # {1: 41.0, 2: 37.0, 3: 30.0} 출력
all_df['Age'] = all_df['Age'].fillna(all_df.Pclass.map(age_dict['Age'])) 
# 1 = 41 , 2= 37 , 3= 30 , #Pclass(1,2,3)과 대응하는 번호에 결측치 평균값을 넣음

# Cabin
all_df["Cabin"].fillna("No Cabin", inplace = True) # 결측치를 No cabin으로 대체
print("Values from Cabin: ", all_df["Cabin"].unique())  # 데이터프레임의 유일한 값 출력
all_df['Cabin_Code'] = all_df['Cabin'].fillna('X').map(lambda x: x[0].strip())
# 결측치의 경우에는 'X', 있을경우에는 맨 앞 글자만 자른다(strip)
print("Values from Cabin Code: ", all_df["Cabin_Code"].unique())

# Fare
print("Avg. Mean:", np.round(all_df['Fare'].mean(), 2))
all_df['Fare'] = all_df['Fare'].fillna(round(all_df['Fare'].mean(), 2))

# Embarked
all_df["Embarked"].fillna("X", inplace = True)
print("Values from Embarked: ", all_df["Embarked"].unique())

# Delete Columns
all_df.drop(['Ticket', 'Cabin', 'Name', 'PassengerId'], axis=1, inplace=True)
print("After Handling:", all_df.shape)
```

### Feature Encoding
* 종류 : Ordianal Encoding , Label Encoding , One-Hot Encoding

* 이유 : 머신러닝은 공식으로 이루어져 있으므로 문자는 받아들이지 않기에 공식의 형태로 바꾸어야 한다.
[자세한 내용](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html)
```python
str_cols = ['Pclass', 'Sex', 'Cabin_Code', 'Embarked'] # 문자열 형태
num_cols = ['Age', 'SibSp', 'Parch', 'Fare', 'Survived'] # 숫자 형태

onehot_df = pd.get_dummies(all_df[str_cols]) # 문자를 가변수의 형태로 변환
print("onehot_df Shape:", onehot_df.shape) 
num_df = all_df[num_cols]
print("num_df Shape:", num_df.shape)

all_cleansed_df = pd.concat([num_df, onehot_df], axis=1) # 데이터프레임을 합친다.
print("all_cleansed_df Shape:", all_df.shape)
```
### 데이터프레임의 분리
* X는 독립 변수
* Y는 종속 변수
* test_data는 제출전에 한번 적용하는 용도
```python
X = all_cleansed_df[:train.shape[0]]
print("X Shape is:", X.shape)
y = X['Survived']
X.drop(['Survived'], axis=1, inplace=True)
test_data = all_cleansed_df[train.shape[0]:].drop(columns=['Survived'])
test_data.info()
```

## scilkit-learn 
[Scilkit-learn](https://scikit-learn.org/stable/user_guide.html)
### 라이브러리 설치
```python
!pip install scikit-learn==0.23.2  # 라이브러리 설치
import sklearn # 라이브러리 불러오기
print(sklearn.__version__) # 설치된 라이브러리 버전 확인
```
### 데이터 섞기
```python
from sklearn.model_selection import train_test_split 


X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, stratify = X[['Pclass']], random_state=42)
X_train.shape, X_val.shape, y_train.shape, y_val.shape
```
train_test_split : 데이터를 train set(학습 데이터 세트), test set(테스트 세트) 두가지로 쉽게 분리할 수 있다.    
test_size : test set의 구성 비율을 나타낸다. 반대는 train_size.  
stratify : 데이터에서 trian과 test를 나눌때 데이터의 분포 비율을 유지    
(ex. 25% 0과 75% 1이면 나누어진 데이터도 25%과 75% 1이 됨)   
random_state : 데이터 분할시 섞이게 되는데 이를 위한 시드값
### Base Model
```python
from sklearn.metrics import accuracy_score # 배열의 정확도를 평가?
def acc_score(y_true, y_pred, **kwargs): # ** kwargs = 함수 호출 키워드 생성
    return accuracy_score(y_true, (y_pred > 0.5).astype(int), **kwargs)
```
```python
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier# 결정트리분류기
from sklearn.metrics import roc_curve, roc_auc_score # 예측곡선과 예측점수
from matplotlib import pyplot as plt

tree_model = DecisionTreeClassifier(max_depth=3)
tree_model.fit(X_train, y_train)

predictions = tree_model.predict_proba(X_val)
AUC = roc_auc_score(y_val, predictions[:,1])
ACC = acc_score(y_val, predictions[:,1])
print("Model AUC:", AUC)
print("Model Accurarcy:", ACC)
print("\n")

fpr, tpr, _ = roc_curve(y_val, predictions[:,1])
# 그래프 그리기 ---- 
fig, ax = plt.subplots(figsize=(10, 6))

ax.plot(fpr, tpr)
ax.text(x = 0.3, 
        y = 0.4, 
        s = "Model AUC is {}\n\nModel Accuracy is {}".format(np.round(AUC, 2), np.round(ACC, 2)), 
        fontsize=16, bbox=dict(facecolor='gray', alpha=0.3))
ax.set_xlabel('FPR')
ax.set_ylabel('TPR')
ax.set_title('ROC curve')

plt.show()
```
![Roc_Curve](https://user-images.githubusercontent.com/65166786/160247161-cb8e6154-952d-47f8-84dd-69e26b9675ea.PNG)

### Helper Class 생성
```python
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, confusion_matrix 
# confusion_matrix = 원래 표본(true), 모형의 예측값(pred)가 일치하는지를 갯수로 센 결과를 표로 나타낸 것
from matplotlib import pyplot as plt

SEED = 0 # for Reproducibility

# class 
class sk_helper(object):
    def __init__(self, model, seed = 0, params={}):
        params['random_state'] = seed
        self.model = model(**params)
        self.model_name = str(model).split(".")[-1][:-2]
        
    # train
    def train(self, X_train, y_train):
        self.model.fit(X_train, y_train)
        
    # predict
    def predict(self, y_val):
        return self.model.predict(y_val)
    
    # inner fit
    def fit(self, x, y):
        return self.model.fit(x, y)
    
    # feature importance
    def feature_importances(self, X_train, y_train):
        return self.model.fit(X_train, y_train).feature_importances_ # feature_importances_ : 변수 중요도
        
    # roc_curve
    def roc_curve_graph(self, X_train, y_train, X_val, y_val): # 그래프 그리기를 메서드로 만든다.
        self.model.fit(X_train, y_train)
        
        print("model_name:", self.model_name)
        model_name = self.model_name
        preds_proba = self.model.predict_proba(X_val)
        preds = (preds_proba[:, 1] > 0.5).astype(int)
        auc = roc_auc_score(y_val, preds_proba[:, 1])
        acc = accuracy_score(y_val, preds)
        confusion = confusion_matrix(y_val, preds)
        print('Confusion Matrix')
        print(confusion)
        print("Model AUC: {0:.3f}, Model Accuracy: {1:.3f}\n".format(auc, acc))
        fpr, tpr, _ = roc_curve(y_val, predictions[:,1])
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.plot(fpr, tpr)
        ax.text(x = 0.3, 
                y = 0.4, 
                s = "Model AUC is {}\n\nModel Accuracy is {}".format(np.round(auc, 2), np.round(acc, 2)), 
                fontsize=16, bbox=dict(facecolor='gray', alpha=0.3))
        ax.set_xlabel('FPR')
        ax.set_ylabel('TPR')
        ax.set_title('ROC curve of {}'.format(model_name), fontsize=16)

        plt.show()
```
```python
%%time
tree_params = {'max_depth': 6}

tree_model = sk_helper(model=DecisionTreeClassifier, seed=SEED, params=tree_params)
tree_model.roc_curve_graph(X_train, y_train, X_val, y_val) 
```
![DecisionTreeClassifier](https://user-images.githubusercontent.com/65166786/160247478-b4267cac-8900-4061-b664-21719cc988e6.png)

```python
%%time
from sklearn.ensemble import RandomForestClassifier # 랜덤트리분류기

rf_params = {
    'n_jobs': -1,
    'n_estimators': 500, # 결정 트리의 개수
     'warm_start': True, 
     #'max_features': 0.2,
    'max_depth': 6,
    'min_samples_leaf': 2,
    'max_features' : 'sqrt',
    'verbose': 1
}

rf_model = sk_helper(model=RandomForestClassifier, seed=SEED, params=rf_params)
rf_model.roc_curve_graph(X_train, y_train, X_val, y_val)
```
![RandomForestClassifier](https://user-images.githubusercontent.com/65166786/160247674-ed76fb1f-3c60-4b31-8e07-9601add2e5d3.png)

```python
%%time 

import lightgbm
from lightgbm import LGBMClassifier # 수평적 확장인 다른 트리기반 알고리즘에 비해, 수직적으로 확장을 한다.
lgb_params = {
    'metric': 'auc',
    'n_estimators': 10000,
    'objective': 'binary',
}

lgb_model = sk_helper(model=LGBMClassifier, seed=SEED, params=lgb_params)
lgb_model.roc_curve_graph(X_train, y_train, X_val, y_val)
```
![LGBMClassifier](https://user-images.githubusercontent.com/65166786/160247809-274c5d65-5b2e-45c7-a5dd-63ab7bf1a72e.png)
### 모델로 그래프 만들기
```python
tree_features = tree_model.feature_importances(X_train, y_train) # 결정트리
rf_features = rf_model.feature_importances(X_train, y_train) # 랜덤트리
lgb_features = lgb_model.feature_importances(X_train, y_train) # 수직확장트리
```
```python
cols = X.columns.values
feature_df = pd.DataFrame({'features': cols, 
                          'Decision Tree': tree_features, 
                          'RandomForest': rf_features, 
                          'LightGBM': lgb_features})
feature_df
```

```python
%matplotlib inline

import seaborn as sb
import matplotlib.pyplot as plt

width = 0.3
x = np.arange(0, len(feature_df.index))

## ax[0] graph
fig, ax = plt.subplots(nrows = 2, ncols = 1, figsize = (16, 16)) # Option sharex=True
ax[0].bar(x - width/2, feature_df['Decision Tree'], color = "#0095FF", width = width)
ax[0].bar(x + width/2, feature_df['RandomForest'], color = "#E6C0B1", width = width)
ax[0].set_xticks(x)
ax[0].set_xticklabels(feature_df['features'], rotation=90)

## ax[0] legend
colors = {'Decision Tree':'#0095FF', 'RandomForest':'#E6C0B1'} 
labels = list(colors.keys())
handles = [plt.Rectangle((0,0),1,1, color=colors[label]) for label in labels]

ax[0].legend(handles, labels, bbox_to_anchor = (0.95, 0.95))
ax[0].set_title("Feature Importance between Decision Tree and RandomForest", fontsize=20)

## ax[1] graph
ax[1].bar(x, feature_df['LightGBM'], color = "#60F09E")
ax[1].set_xticks(x)
ax[1].set_xticklabels(feature_df['features'], rotation=90)
ax[1].set_title("Feature Importance of LightGBM", fontsize=20)

## plt manage
## plt.xticks(x, feature_df['features'], rotation=90)
plt.tight_layout()
plt.show()
```
![image](https://user-images.githubusercontent.com/65166786/160248565-3cf7457e-eb6d-4f29-8242-8643cf453b37.png)


## pycaret
[Pycaret](https://pycaret.gitbook.io/docs/)
### 데이터 전처리
```python
import numpy as np
from datetime import datetime

version = datetime.now().strftime("%d-%m-%Y %H-%M-%S")

def final_submission(model, data, version):
    final_preds = model.predict(data)
    binarizer = np.vectorize(lambda x: 1 if x >= .5 else 0)
    prediction_binarized = binarizer(final_preds)
    submission = pd.concat([sample,pd.DataFrame(prediction_binarized)], axis=1).drop(columns=['Survived'])
    submission.columns = ['PassengerId', 'Survived']
    submission.to_csv('Sklearn of Submit Date {} Submission.csv'.format(version), index=False)
    
final_submission(lgb_model, test_data, version)
```
### 라이브러리 설치
```python
# !pip uninstall scikit-learn -y
!pip install pycaret==2.2.3
```
### 라이브러리 불러오기
```python
# check version
from pycaret.utils import version
import sklearn
print("pycaret version:", version())
print("sklearn version:", sklearn.__version__)
```

```python
all_df_pycaret = pd.concat([X, y], axis=1)
all_df_pycaret['Survived'] = all_df_pycaret['Survived'].astype('int64')
all_df_pycaret.info()
```
### 데이터세트 준비
```python
from pycaret.classification import *

setup(data = all_df_pycaret, 
      target = 'Survived', 
      fold = 3,
      silent = True
     )

set_config('seed', 123
```
### 모델 학습 및 생성
```python
%%time

best_model = compare_models(sort = 'Accuracy', n_select = 3)
```

```python
%%time
gbc_model = create_model('gbc') 
```
gbc는 내장된 모델이다 models()를 치면 내장된 다른 모델들이 나온다.
```python
%%time
tuned_gbc = tune_model(gbc_model, n_iter = 50)
```

```python
plot_model(tuned_gbc, plot = 'feature_all')
```
### 에측(Prediction)
```python
predictions = predict_model(tuned_gbc, data = test_data)
predictions.info()

```
### 제출(저장)
```python
submission = pd.read_csv(BASE_DIR + 'sample_submission.csv')
submission['Survived'] = predictions['Label']
submission.to_csv('PyCaret Submission.csv', index=False)
submission.head()
```