﻿---
title: "﻿BeautifulSoup를 이용한 Python 텍스트 웹 크롤링 및 이미지 웹 크롤링"
excerpt: "네이버 랭킹 뉴스의 제목과 내용을 가져오는 방법을 알아본다."
categories:
- Python
tags:
- [Python]
- [Beautifulsoup]
- [Web crawling]
date: 2022-04-22 14:00:00
---

# 네이버 랭킹뉴스 웹 크롤링
네이버 랭킹 뉴스는 사람들이 어떤 이슈에 가장 많은 관심을 보이는지를 쉽게 파악 할 수 있기에
해당 페이지를 크롤링하면 무언가 의미있지 않을까.. 

## 기사 제목만 가져오기
네이버 랭킹뉴스에 가보면 수많은 기사들의 제목들이 보인다.
![image](https://user-images.githubusercontent.com/65166786/164966778-74f9e95f-553c-464d-91a0-041f6e94c060.png)

이러한 기사들의 제목만 가져오는 텍스트 크롤링을 해본다.

일단 하단의 코드로 네이버 랭킹뉴스를 크롤링 대상으로 잡아준다.
```
import requests  
from bs4 import BeautifulSoup
def main():

headers = { 
'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'  
}
url = "https://news.naver.com/main/ranking/popularDay.naver"
```
그리고 요청을 보낸뒤 파싱을 한다.
```
req = requests.get(url=url, headers=headers)
print(req.status_code) # 200이면 정상이다.
soup = BeautifulSoup(req.text, 'html.parser')
```
랭킹 뉴스는 각 언론사별 카드 뉴스의 형태로 되어 있는데, 이를 분석해보면 
`rankingnews_box`라는 코드로 되어있다는 것을 알 수 있다.
이를 찾아서 `ranking_boxes`라는 변수에 넣어준다.
```
ranking_boxes = soup.find_all('div', 'rankingnews_box')
```
각 언론사별로 기사 제목만 수집하여 `rankingnewstitle` 리스트에 넣는다.
```
rankingnewstitle = [] # 빈 리스트 생성 
for rankingbox in ranking_boxes:  
    article_list = rankingbox.find("ul", "rankingnews_list").find_all("li")  
    for article in article_list:  
        content = article.find("div", "list_content")  
        # 각 언론사별로 기사 제목 수집해서 ranking_news_titles에 삽입  
  if content:  
            rankingnewstitle.append(content.find("a").text)
 ```
  `rankingnewstitle` 리스트를 출력한다.
```
 for title in rankingnewstitle:  
    print(title)
```

완성된 코드
```
import requests  
from bs4 import BeautifulSoup  
  
def main():  
    headers = {  
        'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'  
  }  
    url = 'https://news.naver.com/main/ranking/popularDay.naver'  
  req = requests.get(url=url, headers=headers)  
    print(req.status_code)  
    soup = BeautifulSoup(req.text, 'html.parser')  
    
def crawling(soup):  
    ranking_boxes = soup.find_all('div', 'rankingnews_box')  # class_인 이유는 python 내장변수인 class와 구별 위함  
  rankingnewstitle = []  
    for rankingbox in ranking_boxes:  
        article_list = rankingbox.find("ul", "rankingnews_list").find_all("li")  
        for arti in article_list:  
            content = arti.find("div", "list_content")  
            # 각 언론사별로 기사 제목 수집해서 ranking_news_titles에 삽입  
	 if content:  
	  rankingnewstitle.append(content.find("a").text)  
    for title in rankingnewstitle:  
        print(title)  
if __name__ == "__main__":  
    main()
```
실행 시 이렇게 제목만 가져오는 것을 확인할 수 있다.
![출력 결과](https://user-images.githubusercontent.com/65166786/164975681-cc2a5d8c-6330-44fc-94be-888ed2c54894.png)

## 제목과 내용 둘 다 가져오기
기사 제목과 내용간의 괴리감이 있을 경우를 생각해서, 내용을 가져와보도록 한다.
제목과 달리 내용은 링크를 열어야 볼 수 있으므로, 링크를 연 뒤 내용을 가져오는 코드가 추가로 필요하다.
링크를 저장할 리스트를 추가로 만든 뒤
```
rankinglink = [ ]
```
기사 제목을 수집하는 코드인  `rankingnewstitle.append` 밑에 `rankinglink.append`를 추가하여 링크도 수집하게 만든다.
```
if content:  
            rankingnewstitle.append(content.find("a").text)
	        rankinglink.append(content.find("a")["href"])
```
기사 제목과 내용을 함께 저장할 리스트를 만든다.
```
news  = [ ]
```

```
for title, link in zip(ranking_news_titles, ranking_news_links):  
    url = f"https://news.naver.com{link}" # 링크를 완전하게 만든다.  
  r = requests.get(url, headers=headers)  
    time.sleep(0.5)  # 한 번에 너무 많은 요청이 들어오면 네이버에서 IP차단 시킬수도
 # 이제 기사 원문을 파싱해보자.  
  soup = BeautifulSoup(r.text, "html.parser")  
    article_body = soup.find("div", id="articleBodyContents")  
    article_body = article_body.text.strip()  
    # 파싱한 원문을 articles에 삽입한다. 
  articles.append(  
        {  
            "title": title,  
  "content": article_body  
        }  
    )
```
이러면 결과가 나와야하는데.. 너무 많은 요청을 전송해서
`requests.exceptions.ConnectionError` 에러가 발생했다.
나중에 해결하는걸로..
